{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32cb9d28-d83a-4fbe-94f3-b79f8a07b005",
   "metadata": {},
   "source": [
    "# AI agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6609b494-f64b-401f-96a0-5096aed48165",
   "metadata": {},
   "source": [
    "In this document I demonstrate the methods and tools to create an AI agent, its API and how to use the created agent.\n",
    "The AI agent is created using open source packages with [Python](https://www.python.org/) programming language. To create the agent I used open source packages [Langchain](https://python.langchain.com/docs/introduction/) and [Langgraph](https://langchain-ai.github.io/langgraph/). To create the user interface I utilized  [Gradio](https://www.gradio.app/guides/quickstart)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b167d1a4-bcc1-4bac-9788-59c83374ba84",
   "metadata": {},
   "source": [
    "> Note: In this document I provided more detailed explanations on the code and how to use the agent. For a quick start on using the agent simply run app.py or use the following code in a jupyter notebook cell:\n",
    "\n",
    "```\n",
    "import app\n",
    "\n",
    "app.demo.launch()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1647b51-4f39-4c15-806b-b9145df136b6",
   "metadata": {},
   "source": [
    "## Structure of the created AI agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c5308-19ca-41f4-9888-9b05e783a18e",
   "metadata": {},
   "source": [
    "Here I use OpenAI and Tavily search engine tools. API keys for these packages are required in .env file. To use these packages, you need to create and pass an API key. Once you have those keys, you need to add those to the .env file as following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ec7e6-0f55-4b6b-83a8-0a4c58f87eea",
   "metadata": {},
   "source": [
    "```OPENAI_API_KEY=\"your-api-key\"```\n",
    "\n",
    "```TAVILY_API_KEY=\"your-api-key\"```\n",
   "\n",
   "Important note: An API key is a unique code that identifies your requests to the API. Your API key is intended to be used by you and [must be kept private](https://help.openai.com/en/articles/5008148-can-i-share-my-api-key-with-my-teammate-coworker). See OpenAI's [Best Practices for API Key Safety](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9f6c93-1b5c-4c3f-89cb-5c0dffdfb8ce",
   "metadata": {},
   "source": [
    "We first import the necessarty packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbc9e51c-747f-4c37-aeaa-1f64dc3dc317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import base64\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "# load environment variables/API keys from .env file\n",
    "_ = load_dotenv()\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12407e9-6dce-4020-8c81-0166a930b1d1",
   "metadata": {},
   "source": [
    "Define Large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2215178e-1f28-4724-a5b4-378f877d4122",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf395c-95b2-4412-bea4-6d562be65584",
   "metadata": {},
   "source": [
    "Define tools:\n",
    "The Ai agent is able to perform 3 main tasks as following:\n",
    "1) General chat with the user and answer general questions\n",
    "2) Recommend/Find products based on user's text\n",
    "3) Find products based on user's image\n",
    "\n",
    "To perform the tasks, we need to provide the agent with the proper tools. The agent then processes the user input and based on that takes action and tool calling is performed for the suitable/relevant tool. Here, I assumed that the agent has access to real-time web data specifically from Amazon.ca website for product search, while for answering general questions the agent has access to web data. Let's dive into the tools.\n",
    "\n",
    "**Tool1**:  TavilySearchResults. This tool queries the Tavily Search API and gets back json. We used a search tool instead of creating a regular function with predefined questions and answers, since the user may ask some questions that need accessing web data, such as what is the new trends in clothing this year? What is the weather like in my city?\n",
    "\n",
    "**Tool2** :  For product search we initialized TavilySearch tool with costumized input settings. TavilySearch is a search engine tha is built AI agents (LLMs) to provide real-time search results fast and accurately. Here, we wanted the agent to return maximum of 2 recommendation while accessing only amazon.ca. Search_depth is set to advanced to provide the most relevant results related to the user's query.\n",
    "\n",
    "> Note: Both of these tools can take-in more variables for specific settings. For more details see [`TavilySearchResults`](https://api.python.langchain.com/en/latest/tools/langchain_community.tools.tavily_search.tool.TavilySearchResults.html) and [`TavilySearch`](https://python.langchain.com/docs/integrations/tools/tavily_search/).\n",
    "\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6e8e8-75fd-48df-9c8f-6b4e1ca64850",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool1 = TavilySearchResults(max_results=4) #increased number of results\n",
    "tool2 = TavilySearch(\n",
    "    max_results=2,\n",
    "    include_domains=['amazon.ca'] ,\n",
    "    search_depth = 'advanced',\n",
    ")\n",
    "tools = [tool1, tool2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d754f89a-9d7c-4c79-9e51-e61e4992e267",
   "metadata": {},
   "source": [
    "We define a prompt for the agent that lays out how it uses the large language model as well as the tools to answer a question. The agent understands the perompt and uses the graph structure to perform the tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5804778b-6e7f-47ee-a540-6b4986abffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.datetime.today().strftime(\"%A, %B %d, %Y\")\n",
    "prompt = f\"\"\"You are a smart shopping assistant. Your name is AI assistant. You can look up information.\n",
    "Yo can recommend shoppers about products and help them make decision on shopping.\n",
    "You are allowed to make multiple calls (either together or in sequence).\n",
    "Only look up information when you are sure of what you want.\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that.\n",
    "Only when asked questions about products or recommending products use TavilySearch tool and search only on amazon.ca website, otherwise\n",
    "for general questions about yourself or general topics use TavilySearchResults tool where you have access to various web resources.\n",
    "For questions regarding today's date use {today}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb85751-21c2-48a2-814a-96e363d359e5",
   "metadata": {},
   "source": [
    "We create an in-memory checkpointer and pass it to the agent. This enables the agent to remember the conversations and queries and get back to them, if necessary. The agent is created based on LangGraph's reAct implementation, whic is a pre-built graph structure. The agent uses LLM to think, understand and process information to take any further action, utilize tools and get back to the LLM in an iterative process, if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c2ac2f-35be-458f-b42e-1742945c5cee",
   "metadata": {},
   "source": [
    "> Note: Here we created a thread id for the start of the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f18d9e94-d764-4051-bc0c-4264971606dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an in-memory checkpointer\n",
    "memory = MemorySaver()\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "agent = create_react_agent(llm, tools, prompt=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cce5338-3c98-4fe4-b90b-21faa2a9b44f",
   "metadata": {},
   "source": [
    "The agent is created based on LangGraph's [reAct](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) implementation, whic is a pre-built graph structure. The agent uses LLM to think, understand and process information to take any further action, utilize tools and get back to the LLM in an iterative process, if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dadbc39-8b2a-47ee-8135-8b58d3e47626",
   "metadata": {},
   "source": [
    "Let's visualize the structure of the created agent:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f9dfe5-676b-455a-9b3c-02b9711c3eed",
   "metadata": {},
   "source": [
    "![title](images/agent_struct.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a21ac30-aa79-4318-b8d6-1419f269a8c1",
   "metadata": {},
   "source": [
    "For a simple example, we can invoke the agent to answer a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa9781fb-862f-4074-9f7f-d6827a5360a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is AI assistant. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "message='What is your name?'\n",
    "result = agent.invoke({\"messages\": message},config)\n",
    "print(result['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eccbe41-c0f9-4655-89b8-b10aa79bd4db",
   "metadata": {},
   "source": [
    "However, for interactive agent and being able to pass image and text data, we created a UI using gradio. The UI can be used within jupyter notebook (where you can use it within the notebook or use the created URL) as well as a python IDE (where you can use the created URL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5fb4a98-5257-48c9-9d44-e9f75a53bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load and encode the image\n",
    "def load_and_encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        binary_data = image_file.read()\n",
    "    return base64.b64encode(binary_data).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c58ce2bb-1c94-4161-9b35-ad28af3b17d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(message, history):\n",
    "    if len(message['files'])==0:\n",
    "        message_txt = message['text']\n",
    "        result = agent.invoke({\"messages\": message_txt},config)\n",
    "    else:\n",
    "        image_path = message[\"files\"][0]\n",
    "        image_data = load_and_encode_image(image_path)\n",
    "        messages_im = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": message[\"text\"]},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n",
    "            },\n",
    "        ])\n",
    "        result = agent.invoke({\"messages\": messages_im},config)\n",
    "    return result['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e81ff52b-2edf-469d-b2ed-9a8d0089927d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.ChatInterface(\n",
    "    run_agent,\n",
    "    type=\"messages\",\n",
    "    multimodal=True,\n",
    "    textbox=gr.MultimodalTextbox(file_count=\"single\", file_types=[\"image\"], sources=[\"upload\"]),\n",
    "    title=\"AI assistant\",\n",
    "    description=\"Hi there! Ask me any question!\",\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465f8d5-6060-47aa-aaa9-51b68f194129",
   "metadata": {},
   "source": [
    "Here is a snapshot of a sample result:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b90811-2212-4e63-abfb-9e225e802227",
   "metadata": {},
   "source": [
    "![title](images/result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5fc91f-8f0f-4f47-b536-7f502d0f63e3",
   "metadata": {},
   "source": [
    "We can then close the launch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f367ac1c-e385-4c5e-8508-0fe60b0445e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74204236-99fe-4e05-9c1f-bc9f734ed100",
   "metadata": {},
   "source": [
    "# Agent's functions descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90fe5ad-4149-47ef-8024-49ce0cb75f05",
   "metadata": {},
   "source": [
    "I used docstring for the created functions in the AI agent code in the app.py file. We can see the descriptions of the script as well as the created functions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84777479-a45d-468b-a9cb-fc901d698aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Agent\n",
      "This script creates an AI agent and launches gradio user interface for\n",
      "chatting with the agent and enquirying info and products recommendations.\n",
      "This file can be run as the main file. It can also be imported as a module and contains the following\n",
      "functions:\n",
      "\n",
      "    * load_and_encode_image\n",
      "    * run_agent\n",
      "Example of importing in a jupyter notebook:\n",
      "    >>>import app\n",
      "    >>>app.demo.launch()\n",
      "This will create the UI within the notebook. A local url will also be created to\n",
      "use the UI in a browser.\n",
      "\n",
      "Closing the launch:\n",
      "    >>>app.demo.close()\n",
      "or simply restart the kernel in the notebook.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import app\n",
    "print(app.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2d7a4c4-2f1d-4b78-965c-22dc7d6cc027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is a function that guides the response of the chatbot based on\n",
      "user \"message\" and \"history\" of the chat. Please note that here we are\n",
      "using the agent's memory, since .\n",
      "\n",
      "Note: history becomes important when dealing with simple functions or an LLM.\n",
      "For example, when invoking a langchain model only, we can append the messages in\n",
      "history to a list of chat history and pass it to the model.\n",
      "See as an example here <https://www.gradio.app/guides/chatinterface-examples#lang-chain>\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "message : dict(text: str, files: list[filepath])\n",
      "    The input value representing the user's most recent message.\n",
      "    When the user types in and/or attaches an image, message is captured\n",
      "    in the \"parameter_0\" Multimodaltextbox component.\n",
      "history : list\n",
      "    A list of openai-style dictionaries with role and content keys,\n",
      "    representing the previous conversation history. \n",
      "\n",
      "Returns\n",
      "-------\n",
      "Str\n",
      "    The agent's final answer to the user's query. This could be a short\n",
      "    answer to a question, a list of recommendations with links, or asking\n",
      "    further questions for clarification.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(app.run_agent.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c863ae9f-8319-42b0-9be5-088521acdb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This function takes in the path of the uploaded image, loads and encodes\n",
      "the binary data into Base64 and then decode it back to UTF-8.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "image_path : str\n",
      "    This is the filepath of the image.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "str\n",
      "    Decoded UTF-8 string. \n",
      "Note: This can then be used for creating \"image_url\" to be passed to\n",
      "the created AI agent in run_agent function.\n",
      "\n",
      "Example\n",
      "-------\n",
      "Convert an image to decoded UTF-8 string\n",
      "\n",
      ">>> image_path = \"red_shirt.jpg\"\n",
      ">>> image_data = load_and_encode_image(image_path)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(app.load_and_encode_image.__doc__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
